import torch              # 导入 PyTorch 库
from typing import Tuple  # 导入Tuple类型注解，用于表示一个元组类型 

"""
这里直接引用了 LLaMA 中对旋转位置编码的官方实现(https://github.com/meta-llama/llama3/blob/main/llama/model.py).
在 LLaMA 中，旋转位置编码是通过预先计算的方式实现的，而不是在训练过程中动态计算。具体实现过程为:
(1) 基于 precompute_freqs_cis(dim, end, theta) 函数预先计算(足够长的)位置编码的复数形式笛卡尔坐标 freqs_cis.
    - 计算旋转角度;
    - 创建一个长度为end(序列的最大长度)的张量, 其中的每个元素为其所在位置序号, 即从0到end-1;
    - 计算位置序号与旋转角度的乘积;
    - 将上一步计算的极坐标转换为笛卡尔坐标.
(2) 基于 apply_rotary_emb(xq, xk, freqs_cis) 函数将预先计算好的位置编码 freqs_cis 应用到输入张量  xq 和 xk 上:
    - 将 xq 和 xk 转换为复数形式;
    - 调用 reshape_for_broadcast(freqs_cis, x) 函数将 freqs_cis 变形为与上一步 xq 转换结果一致的形状;
    - 计算并输出 xq 和 xk 的旋转嵌入.
"""

def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):
    """
    预先计算位置编码的复数形式笛卡尔坐标.
    Args:
        dim (int): 输入维度大小, 可以理解成embedding的维度.
        end (int, optional): 结束位置索引, 为序列的最大长度.
        theta (float, optional): 控制频率缩放的超参数，默认为 10000.0(原论文推荐值为10000).
    Returns:
        torch.Tensor: 预计算的位置编码复数向量. 
                      向量形状为 (end, dim//2), 第2维为输入维度的1/2, 因为旋转是以2维矩阵为单位的, 即每两个数有一个旋转角度.
    """
    # (1) 计算旋转角度
    # 其中的第 i 个元素的计算方法为: 1 / (theta ** (2 * (i - 1) / dim)), i 从 1 到 dim/2.
    # 例如(即下面执行的例子) dim=8, theta=10000.0, 则得到的 freqs = tensor([1.0000, 0.1000, 0.0100, 0.0010])
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)) 

    # (2) 创建一个长度为end(序列的最大长度)的张量, 其中的每个元素为其所在位置序号, 即从0到end-1.
    # 例如(即下面执行的例子) end=5, 则得到的 t = tensor([0, 1, 2, 3, 4])
    t = torch.arange(end, device=freqs.device)

    # (3) 计算位置序号与旋转角度的乘积(外积, 其中的元素是t中的每个元素与freqs中的每个元素相乘的结果)
    # 得到的结果是: freqs = tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],
    #                             [1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03],
    #                             [2.0000e+00, 2.0000e-01, 2.0000e-02, 2.0000e-03],
    #                             [3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03],
    #                             [4.0000e+00, 4.0000e-01, 4.0000e-02, 4.0000e-03]])
    freqs = torch.outer(t, freqs).float()
    
    # (4) 将上一步计算的极坐标(r=1, theta)转换为笛卡尔坐标(用复数表示，实部为cos(freq)，虚部为sin(freq))
    # 得到的结果是: freqs_cis = tensor([[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],
    #                                 [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],
    #                                 [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],
    #                                 [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],
    #                                 [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j]])
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    
    return freqs_cis

def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):
    """
    根据输入张量 x 将预先计算好的旋转角度 freqs_cis 修改成特定形状.
    Args:
        freqs_cis (torch.Tensor): 预先计算好的旋转角度.
        x (torch.Tensor): 输入张量.
    Returns:
        torch.Tensor: 变形后的freqs_cis. freqs_cis的维数会变为和x一致, 其中第2维是freqs_cis.shape[0], 最后1维是freqs_cis.shape[1], 其余维度为1
    """
    ndim = x.ndim
    assert 0 <= 1 < ndim
    assert freqs_cis.shape == (x.shape[1], x.shape[-1])
    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]

    return freqs_cis.view(*shape)


# 旋转位置嵌入
def apply_rotary_emb(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    用预先计算的角度来旋转输入的xq和xk.
    Args:
        xq (torch.Tensor): query张量
        xk (torch.Tensor): key张量
        freqs_cis:  (torch.Tensor): 预计算的位置编码复数向量
    Returns:
        xq_out (torch.Tensor): 经过旋转嵌入后的query张量，形状与 xq 相同
        xk_out (torch.Tensor): 经过旋转嵌入后的key张量，形状与 xk 相同
    """
    freqs_cis = freqs_cis.to(xq.device)
    # (1) 将xq和xk转换为复数形式
    # 使用后面的例子对转换过程进行一步步拆解:
    # a. *xq.shape[:-1]是一个解包操作，它将xq的形状(2, 5, 2, 8)除了最后一个维度展开成一系列参数, 即得到 2, 5, 2
    # b. xq.float().reshape(*xq.shape[:-1], -1, 2)=xq.float().reshape(2, 5, 2, -1, 2), 
    #    即将xq的形状从(2, 5, 2, 8)变为(2, 5, 2, 4, 2), 其中-1表示自动计算该维度的大小, 即4.
    #    也即是说, xq的每个头的嵌入维度 8 被拆分为 4 组, 每组 2 个维度.
    # c. torch.view_as_complex() 实数张量转换为复数张量, 它期望输入的张量最后一个维度的大小是2, 如[a, b] 则对应输出 a + bi.
    #    最终的输出结果形状是(2, 5, 2, 4).
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    # 转换结果: xq_ = tensor([[[[  0.+1.j,   2.+3.j,   4.+5.j,   6.+7.j],
    #                          [  8.+9.j,  10.+11.j,  12.+13.j,  14.+15.j]],
    #                         [[ 16.+17.j,  18.+19.j,  20.+21.j,  22.+23.j],
    #                          [ 24.+25.j,  26.+27.j,  28.+29.j,  30.+31.j]],
    #                         [[ 32.+33.j,  34.+35.j,  36.+37.j,  38.+39.j],
    #                          [ 40.+41.j,  42.+43.j,  44.+45.j,  46.+47.j]],
    #                         [[ 48.+49.j,  50.+51.j,  52.+53.j,  54.+55.j],
    #                          [ 56.+57.j,  58.+59.j,  60.+61.j,  62.+63.j]],
    #                         [[ 64.+65.j,  66.+67.j,  68.+69.j,  70.+71.j],
    #                          [ 72.+73.j,  74.+75.j,  76.+77.j,  78.+79.j]]],
    #                        [[[ 80.+81.j,  82.+83.j,  84.+85.j,  86.+87.j],
    #                          [ 88.+89.j,  90.+91.j,  92.+93.j,  94.+95.j]],
    #                         [[ 96.+97.j,  98.+99.j, 100.+101.j, 102.+103.j],
    #                          [104.+105.j, 106.+107.j, 108.+109.j, 110.+111.j]],
    #                         [[112.+113.j, 114.+115.j, 116.+117.j, 118.+119.j],
    #                          [120.+121.j, 122.+123.j, 124.+125.j, 126.+127.j]],
    #                         [[128.+129.j, 130.+131.j, 132.+133.j, 134.+135.j],
    #                          [136.+137.j, 138.+139.j, 140.+141.j, 142.+143.j]],
    #                         [[144.+145.j, 146.+147.j, 148.+149.j, 150.+151.j],
    #                          [152.+153.j, 154.+155.j, 156.+157.j, 158.+159.j]]]])
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    # 转换结果: xk_ = tensor([[[[ 0.+1.j,  2.+3.j,  4.+5.j,  6.+7.j]],
    #                         [[ 8.+9.j, 10.+11.j, 12.+13.j, 14.+15.j]],
    #                         [[16.+17.j, 18.+19.j, 20.+21.j, 22.+23.j]],
    #                         [[24.+25.j, 26.+27.j, 28.+29.j, 30.+31.j]],
    #                         [[32.+33.j, 34.+35.j, 36.+37.j, 38.+39.j]]],
    #                        [[[40.+41.j, 42.+43.j, 44.+45.j, 46.+47.j]],
    #                         [[48.+49.j, 50.+51.j, 52.+53.j, 54.+55.j]],
    #                         [[56.+57.j, 58.+59.j, 60.+61.j, 62.+63.j]],
    #                         [[64.+65.j, 66.+67.j, 68.+69.j, 70.+71.j]],
    #                         [[72.+73.j, 74.+75.j, 76.+77.j, 78.+79.j]]]])
    
    # (2) 根据 xq_ 将预先计算好的旋转角度 freqs_cis 修改成特定形状(与xq_中每个批次、每个头对应的张量形状一样).
    # 如例子中, xq_ 的形状为 (2, 5, 2, 4), 则 freqs_cis 的形状为 (1, 5, 1, 4).
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_) 
    # 转换结果: freqs_cis = tensor([[[[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j]],
    #                               [[ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j]],
    #                               [[-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j]],
    #                               [[-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j]],
    #                               [[-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j]]]])
    
    # (3) 计算 xq 和 xk 的旋转嵌入
    # 计算过程:
    # a. xq_ * freqs_cis 是一个复数乘法, 它将 xq_ 中的每个元素与 freqs_cis 中的对应元素相乘.
    #    例如, xq_ 中位置为 (0, 1, 1, 2) 的元素为 28.+29.j, 需要与 freqs_cis 中位于 (0, 1, 0, 2) 的元素 0.9999+0.0100j 相乘(由
    #    于小数位数原因, 实际上这个元素是 0.99995+0.009999833j)它们的乘积为 27.7086+29.2785j.
    # b. torch.view_as_real() 是一个函数, 它将复数张量转换为实数张量. 
    #    如果复数张量中的元素是 a + bi, 那转换后的实数张量是一个形如 [a, b] 的向量
    # c. flatten(3) 方法将张量在第3维上进行展平, 即把第3维及其之后的所有维度展平成一个维度.
    #    例如, 上面 torch.view_as_real(xq_ * freqs_cis) 操作得到的结果形状是 (2, 5, 2, 4, 2), 
    #    flatten(3) 操作之后的张量形状是 (2, 5, 2, 8).
    #    此时, 根据上面的计算, 得到的 xq_out 中位置在 (0, 1, 1, 4:6) 的元素应该是 [27.7086, 29.2785]. 如下所示:
    #    xq_out = tensor([[[[   0.0000,    1.0000,    2.0000,    3.0000,    4.0000,    5.0000,    6.0000,    7.0000],
    #                       [   8.0000,    9.0000,   10.0000,   11.0000,   12.0000,   13.0000,   14.0000,   15.0000]],
    #                      [[  -5.6602,   22.6487,   16.0132,   20.7021,   19.7890,   21.1989,   21.9770,   23.0220],
    #                       [  -8.0695,   33.7029,   23.1746,   29.4608,   27.7086,   29.2785,   29.9690,   31.0300]],
    #                      [[ -43.3235,   15.3647,   26.3688,   41.0571,   35.2528,   37.7126,   37.9219,   39.0759],
    #                       [ -53.9271,   19.3099,   32.6200,   50.4870,   43.0913,   45.8709,   45.9059,   47.0919]],
    #                      [[ -54.4345,  -41.7359,   32.6953,   63.4982,   50.3868,   54.5359,   53.8348,   55.1618],
    #                       [ -63.4834,  -48.5269,   37.9738,   73.5050,   58.1433,   62.7723,   61.8107,   63.1857]],
    #                      [[   7.3590,  -90.9222,   34.6990,   87.4127,   65.1863,   71.6641,   69.7154,   71.2794],
    #                       [   8.1842, -102.2058,   38.9521,   97.8965,   72.8600,   79.9776,   77.6834,   79.3114]]],
    #                     [[[  80.0000,   81.0000,   82.0000,   83.0000,   84.0000,   85.0000,   86.0000,   87.0000],
    #                       [  88.0000,   89.0000,   90.0000,   91.0000,   92.0000,   93.0000,   94.0000,   95.0000]],
    #                      [[ -29.7537,  133.1905,   87.6269,  108.2891,   98.9850,  101.9949,  101.8969,  103.1020],
    #                       [ -32.1630,  144.2447,   94.7883,  117.0478,  106.9046,  110.0745,  109.8889,  111.1099]],
    #                      [[-149.3591,   54.8167,   88.8806,  135.3560,  113.6370,  119.2964,  117.7618,  119.2358],
    #                       [-159.9626,   58.7619,   95.1318,  144.7859,  121.4754,  127.4548,  125.7457,  127.2517]],
    #                      [[-144.9235, -109.6457,   85.4806,  163.5667,  127.9512,  136.8996,  133.5944,  135.4014],
    #                       [-153.9724, -116.4366,   90.7591,  173.5736,  135.7076,  145.1359,  141.5704,  143.4254]],
    #                      [[  15.6117, -203.7579,   77.2304,  192.2510,  141.9232,  154.7992,  149.3948,  151.5988],
    #                       [  16.4370, -215.0414,   81.4836,  202.7349,  149.5969,  163.1128,  157.3627,  159.6307]]]])
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    #    xk_out = tensor([[[[   0.0000,    1.0000,    2.0000,    3.0000,    4.0000,    5.0000,    6.0000,    7.0000]],
    #                      [[  -3.2508,   11.5945,    8.8519,   11.9434,   11.8694,   13.1193,   13.9850,   15.0140]],
    #                      [[ -22.1164,    7.4743,   13.8665,   22.1973,   19.5760,   21.3958,   21.9540,   23.0440]],
    #                      [[ -27.2878,  -21.3629,   16.8597,   33.4776,   27.1175,   29.8268,   29.9069,   31.0899]],
    #                      [[   4.0579,  -45.7879,   17.6864,   45.4774,   34.4916,   38.4100,   37.8437,   39.1517]]],
    #                     [[[  40.0000,   41.0000,   42.0000,   43.0000,   44.0000,   45.0000,   46.0000,   47.0000]],
    #                      [[ -15.2976,   66.8654,   44.6587,   55.7369,   51.4674,   53.5173,   53.9450,   55.0540]],
    #                      [[ -75.1342,   27.2003,   45.1224,   69.3467,   58.7681,   62.1877,   61.8739,   63.1239]],
    #                      [[ -72.5323,  -55.3178,   43.2524,   83.5119,   65.8997,   71.0087,   69.7867,   71.2097]],
    #                      [[   8.1842, -102.2058,   38.9521,   97.8965,   72.8600,   79.9776,   77.6834,   79.3114]]]])
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    # 对比 xk 和 xk_out, 可以看出, 旋转位置编码的嵌入, 在第1个token(即位置为0)上, 与原始的嵌入是一致的;
    # 在后面的token上, 嵌入发生了变化. 其中嵌入向量中越靠前的元素变化程度越大，而越靠后的元素变化程度越小. 
    # 不同样本同一个位置的token，旋转角度是相同的. 
    
    return xq_out.type_as(xq), xk_out.type_as(xk)

if __name__ == "__main__":
    # import sys
    # import os
    # # 获取当前文件的绝对路径，并导航到其父目录
    # current = os.path.dirname(os.path.abspath(__file__))
    # parent = os.path.dirname(current)
    # sys.path.append(parent)
    # from model_config import TiaoyuConfig
    # tiaouy_config = TiaoyuConfig()

    import numpy as np
    # import paddle
    
    # (1) 设置一些模拟参数
    batch_size = 2         # 批量大小
    seq_len = 5            # 文本块长度
    embed_dim = 16         # 模型的嵌入维度
    head_num = 2           # 多头自注意力机制中头的个数(即Query的头的个数)
    query_group_num = 2    # 多头自注意力机制中的分组数量(Query被分为query_group_num个组, 每组对应1个Key和Value)
    RoPE_theta = 10000.0   # 旋转位置编码中使用的theta值

    # (2) 构造模拟的输入张量xq、xk
    # 其中xq是一个形状为(2, 5, 2, 8)的张量，代表2个样本(批量大小)，每个样本有5个token(文本块长度)，每个token有2个头，每个头的嵌入维度为8.
    # 如, xq = tensor([[[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.],                   \                       
    #                    [  8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.]],                   \                       
    #                   [[ 16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.],                     \                     
    #                    [ 24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.]],                     \                     
    #                   [[ 32.,  33.,  34.,  35.,  36.,  37.,  38.,  39.],                       |-- 1个样本(有5个token)                   
    #                    [ 40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.]],                     /                     
    #                   [[ 48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.],                     /                     
    #                    [ 56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.]],                   /                       
    #                   [[ 64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.],                   /                       
    #                    [ 72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.]]],                /                          
    #                  [[[ 80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.],    1 \ 1个token   \                 
    #                    [ 88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.]],   1 / 有2个头      \                      
    #                   [[ 96.,  97.,  98.,  99., 100., 101., 102., 103.],    2                \                   
    #                    [104., 105., 106., 107., 108., 109., 110., 111.]],   2                 \                  
    #                   [[112., 113., 114., 115., 116., 117., 118., 119.],    3 -1个头, 有8个维度  |-- 1个样本(有5个token)
    #                    [120., 121., 122., 123., 124., 125., 126., 127.]],   3                 /                  
    #                   [[128., 129., 130., 131., 132., 133., 134., 135.],    4                /                   
    #                    [136., 137., 138., 139., 140., 141., 142., 143.]],   4               /                    
    #                   [[144., 145., 146., 147., 148., 149., 150., 151.],    5              /                    
    #                    [152., 153., 154., 155., 156., 157., 158., 159.]]]]) 5             /                        
    xq = torch.arange(batch_size * seq_len * embed_dim, dtype=torch.float32).reshape(batch_size, seq_len, head_num, embed_dim // head_num)
    # 其中xk是一个形状为(2, 5, 1, 8)的张量，代表2个样本(批量大小)，每个样本有5个token(文本块长度)，每个token有1个头，每个头的嵌入维度为8.
    # 如, xk = tensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.]],  -- 1个token有1个头 \
    #                   [[ 8.,  9., 10., 11., 12., 13., 14., 15.]],  -- 1个头有8个维度   \
    #                   [[16., 17., 18., 19., 20., 21., 22., 23.]],                     \-- 2个样本
    #                   [[16., 17., 18., 19., 20., 21., 22., 23.]],                     /-- 有5个token
    #                   [[24., 25., 26., 27., 28., 29., 30., 31.]],                    /
    #                   [[32., 33., 34., 35., 36., 37., 38., 39.]]],                  /
    #                  [[[40., 41., 42., 43., 44., 45., 46., 47.]],
    #                   [[48., 49., 50., 51., 52., 53., 54., 55.]],
    #                   [[56., 57., 58., 59., 60., 61., 62., 63.]],
    #                   [[64., 65., 66., 67., 68., 69., 70., 71.]],
    #                   [[72., 73., 74., 75., 76., 77., 78., 79.]]]])
    xk = torch.arange(batch_size * seq_len * embed_dim // query_group_num, 
                      dtype=torch.float32).reshape(batch_size, 
                                                   seq_len, 
                                                   head_num // query_group_num, 
                                                   embed_dim // head_num)
    
    # (3) 预先计算位置编码的复数形式笛卡尔坐标
    freqs_cis = precompute_freqs_cis(dim=embed_dim // head_num, end=seq_len, theta=RoPE_theta)
    # 输出结果: tensor([[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j], \ -- 列数为嵌入维度 8 的 1/2
    #                  [ 0.5403+0.8415j,  0.9950+0.0998j,  0.9999+0.0100j,  1.0000+0.0010j],  \
    #                  [-0.4161+0.9093j,  0.9801+0.1987j,  0.9998+0.0200j,  1.0000+0.0020j],   |--行数为文本块长度(5个token)
    #                  [-0.9900+0.1411j,  0.9553+0.2955j,  0.9996+0.0300j,  1.0000+0.0030j],  /
    #                  [-0.6536-0.7568j,  0.9211+0.3894j,  0.9992+0.0400j,  1.0000+0.0040j]])/
    
    np.set_printoptions(precision=10)
    print(freqs_cis.numpy()) # 可以保留10位小数, 用来验证计算结果是否正确

    # (4) 用预先计算的角度来对xq和xk进行旋转位置嵌入
    xq_out, xk_out = apply_rotary_emb(xq, xk, freqs_cis)
    print(f'xq_out:\n{xq_out}')
    print(f'xk_out:\n{xk_out}')